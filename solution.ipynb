{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMO 3 - Diversity Ensemble Solver\n",
    "# Implements \"The Architect\", \"The Hacker\", and \"The Formalist\"\n",
    "# to break the correlation of errors in standard ensembles.\n",
    "\n",
    "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import os, sys, subprocess, gc, re, math, time, queue, threading, contextlib\n",
    "from typing import Optional\n",
    "from jupyter_client import KernelManager\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from openai import OpenAI\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName, load_harmony_encoding, SystemContent, ReasoningEffort, \n",
    "    ToolNamespaceConfig, Author, Message, Role, TextContent, Conversation\n",
    ")\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "def set_env(input_archive, temp_dir):\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-index', '--find-links', f'{temp_dir}/wheels', \n",
    "                    'unsloth', 'trl', 'vllm', 'openai_harmony'], check=True)\n",
    "\n",
    "set_env('/kaggle/input/aimo-3-utils/wheels.tar.gz', '/kaggle/tmp/setup')\n",
    "\n",
    "for k, v in [('TRANSFORMERS_NO_TF', '1'), ('TRANSFORMERS_NO_FLAX', '1'), \n",
    "             ('CUDA_VISIBLE_DEVICES', '0'), ('TOKENIZERS_PARALLELISM', 'false'),\n",
    "             ('TRITON_PTXAS_PATH', '/usr/local/cuda/bin/ptxas'),\n",
    "             ('TIKTOKEN_ENCODINGS_BASE', '/kaggle/tmp/setup/tiktoken_encodings')]:\n",
    "    os.environ[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff97030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # -------------------------------------------------------------------------\n",
    "    # DIVERSITY PROMPTS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # 1. The Architect (Standard Chain-of-Thought)\n",
    "    prompt_architect = (\n",
    "        'You are an elite mathematical problem solver. Your goal is to find the correct answer '\n",
    "        'through rigorous reasoning. \\n'\n",
    "        '- Use Python to verify small steps.\\n'\n",
    "        '- Break down the problem into sub-components.\\n'\n",
    "        '- Output the final integer in \\\\boxed{}.'\n",
    "    )\n",
    "    \n",
    "    # 2. The Hacker (Brute Force / Simulation) - High Temperature\n",
    "    prompt_hacker = (\n",
    "        'You are a Python expert specializing in brute-force search and simulation. \\n'\n",
    "        'Do not waste time on elegant proofs. Focus on computational attacks.\\n'\n",
    "        '1. Translate the problem into a checkable Python function `is_solution(x)`. \\n'\n",
    "        '2. Write an efficient loop or search algorithm to find the answer. \\n'\n",
    "        '3. If the space is infinite, check the first 1,000,000 cases or use Monte Carlo. \\n'\n",
    "        '4. Print the final answer found by your code inside \\\\boxed{}.'\n",
    "    )\n",
    "    \n",
    "    # 3. The Formalist (Symbolic Algebra) - Low Temperature\n",
    "    prompt_formalist = (\n",
    "        'You are a theoretical mathematician. You distrust floating point arithmetic. \\n'\n",
    "        '1. Model every constraint using `sympy` symbols, Equations, and Solvers. \\n'\n",
    "        '2. Use `sympy.ntheory` for number theory and `sympy.geometry` for geometry. \\n'\n",
    "        '3. Avoid loops. seek closed-form analytical solutions using Python as a proof assistant. \\n'\n",
    "        '4. Output the exact integer answer inside \\\\boxed{}.'\n",
    "    )\n",
    "    \n",
    "    # Role Schedule: Flattened list of roles to cycle through for each attempt\n",
    "    # We prioritize Hacker for speed/finding numerical tricks, Architect for general coverage.\n",
    "    attempts = 12 \n",
    "    active_roles = [\n",
    "        prompt_hacker, prompt_architect, prompt_formalist, \n",
    "        prompt_hacker, prompt_architect, prompt_formalist,\n",
    "        prompt_hacker, prompt_architect, prompt_architect,\n",
    "        prompt_hacker, prompt_architect, prompt_formalist\n",
    "    ]\n",
    "\n",
    "    tool_prompt = (\n",
    "        'Use this tool to execute Python code. The environment is stateful. '\n",
    "        'Always use print() to see output.'\n",
    "    )\n",
    "    \n",
    "    served_model_name = 'gpt-oss'\n",
    "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
    "    kv_cache_dtype, dtype = 'fp8_e4m3', 'auto'\n",
    "    \n",
    "    notebook_limit = 17400\n",
    "    base_problem_timeout = 300\n",
    "    high_problem_timeout = 900\n",
    "    \n",
    "    context_tokens = 65536\n",
    "    temperature = 0.9 # Base temp, modifiers applied in solver\n",
    "    workers = 16\n",
    "    turns = 60 # Shorter turns needed for Hacker, longer for Architect\n",
    "    batch_size = 256\n",
    "    \n",
    "    seed = 42\n",
    "    min_p = 0.02\n",
    "    gpu_memory_utilization = 0.95\n",
    "    \n",
    "    # Speed optimizations\n",
    "    search_tokens = 32\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b50ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIMO3Template:\n",
    "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
    "        return (\n",
    "            SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
    "            .with_tools(tool_config)\n",
    "        )\n",
    "\n",
    "    def apply_chat_template(self, system_prompt: str, user_prompt: str, tool_config: ToolNamespaceConfig) -> list[Message]:\n",
    "        system_content = self.get_system_content(system_prompt, tool_config)        \n",
    "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
    "        return [system_message, user_message]\n",
    "\n",
    "class AIMO3Sandbox:\n",
    "    _port_lock, _next_port = threading.Lock(), 50000\n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count=5):\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "            return ports\n",
    "    \n",
    "    def __init__(self, timeout):\n",
    "        self._default_timeout, self._owns_kernel, self._client, self._km = timeout, False, None, None\n",
    "        ports = self._get_next_ports(5)\n",
    "        env = os.environ.copy()\n",
    "        env.update({'PYDEVD_DISABLE_FILE_VALIDATION': '1', 'PYDEVD_WARN_EVALUATION_TIMEOUT': '0',\n",
    "                   'JUPYTER_PLATFORM_DIRS': '1', 'PYTHONWARNINGS': 'ignore', 'MPLBACKEND': 'Agg'})\n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port, self._km.iopub_port, self._km.stdin_port, self._km.hb_port, self._km.control_port = ports\n",
    "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
    "        self._owns_kernel = True\n",
    "        self.execute('import math, numpy, sympy, mpmath, itertools, collections\\nmpmath.mp.dps = 64\\n')\n",
    "    \n",
    "    def execute(self, code, timeout=None):\n",
    "        effective_timeout = timeout or self._default_timeout\n",
    "        msg_id = self._client.execute(code, store_history=True, allow_stdin=False, stop_on_error=False)\n",
    "        stdout, stderr, start = [], [], time.time()\n",
    "        while True:\n",
    "            if time.time() - start > effective_timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "                return f'[ERROR] Execution timed out'\n",
    "            try:\n",
    "                msg = self._client.get_iopub_msg(timeout=1.0)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id: continue\n",
    "            mt, c = msg.get('msg_type'), msg.get('content', {})\n",
    "            if mt == 'stream': (stdout if c.get('name') == 'stdout' else stderr).append(c.get('text', ''))\n",
    "            elif mt == 'error': stderr.append(str(c.get('traceback', [])))\n",
    "            elif mt in {'execute_result', 'display_data'}: \n",
    "                if txt := c.get('data', {}).get('text/plain'): stdout.append(txt)\n",
    "            elif mt == 'status' and c.get('execution_state') == 'idle': break\n",
    "        return ''.join(stdout) if stdout else (''.join(stderr) if stderr else '[WARN] No output')\n",
    "    \n",
    "    def close(self):\n",
    "        if self._owns_kernel and self._km: self._km.shutdown_kernel(now=True)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.execute('%reset -f\\nimport math, numpy, sympy, mpmath, itertools, collections\\nmpmath.mp.dps = 64\\n')\n",
    "\n",
    "class AIMO3Tool:\n",
    "    def __init__(self, timeout, prompt, sandbox):\n",
    "        self.timeout, self.prompt, self.sandbox = timeout, prompt, sandbox\n",
    "    \n",
    "    @property\n",
    "    def tool_config(self): \n",
    "        return ToolNamespaceConfig(name='python', description=self.prompt, tools=[])\n",
    "    \n",
    "    def process_sync_plus(self, message):\n",
    "        code = message.content[0].text\n",
    "        if 'print' not in code.splitlines()[-1] and not 'import' in code.splitlines()[-1]: \n",
    "             code += f'\\nprint({code.splitlines()[-1]})'\n",
    "        out = self.sandbox.execute(code)\n",
    "        auth = Author(role=Role.TOOL, name='python')\n",
    "        return [Message(author=auth, content=[TextContent(text=out)]).with_recipient('assistant')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiversitySolver:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.port = 8000\n",
    "        self.template = AIMO3Template() \n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "        self._start_server()\n",
    "        self.client = OpenAI(base_url=f'http://localhost:{self.port}/v1', api_key='sk-local', timeout=900)\n",
    "        self.sandbox_pool = queue.Queue()\n",
    "        self._init_sandboxes()\n",
    "        self.time_start = time.time()\n",
    "        self.problems_remaining = 50\n",
    "        \n",
    "    def _start_server(self):\n",
    "        # Eagle3 speculative decoding config\n",
    "        spec_config = (\n",
    "            '{\"method\":\"eagle3\",'\n",
    "            '\"model\":\"/kaggle/input/download-eagle3/wenliang1990/gpt-oss-120b-eagle3-aimo3\",'\n",
    "            '\"num_speculative_tokens\":3,'\n",
    "            '\"draft_tensor_parallel_size\":1}'\n",
    "        )\n",
    "        cmd = [sys.executable, '-m', 'vllm.entrypoints.openai.api_server',\n",
    "               '--seed', str(self.cfg.seed),\n",
    "               '--model', self.cfg.model_path, '--served-model-name', self.cfg.served_model_name,\n",
    "               '--tensor-parallel-size', '1', '--max-num-seqs', str(self.cfg.batch_size),\n",
    "               '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization), '--port', str(self.port),\n",
    "               '--dtype', self.cfg.dtype, '--kv-cache-dtype', self.cfg.kv_cache_dtype,\n",
    "               '--max-model-len', str(self.cfg.context_tokens), '--disable-log-stats', \n",
    "               '--enable-prefix-caching', \"--speculative-config\", spec_config]\n",
    "        \n",
    "        self.proc = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, start_new_session=True)\n",
    "        \n",
    "        # Wait for server\n",
    "        print(\"Waiting for server...\")\n",
    "        start = time.time()\n",
    "        while time.time() - start < 180:\n",
    "            try:\n",
    "                self.client.models.list()\n",
    "                print(f\"Server ready in {time.time()-start:.1f}s\")\n",
    "                return\n",
    "            except:\n",
    "                time.sleep(2)\n",
    "        raise RuntimeError(\"Server timeout\")\n",
    "        \n",
    "    def _init_sandboxes(self):\n",
    "        # Initialize in parallel\n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as ex:\n",
    "            futs = [ex.submit(AIMO3Sandbox, 6) for _ in range(self.cfg.workers)]\n",
    "            for f in as_completed(futs):\n",
    "                self.sandbox_pool.put(f.result())\n",
    "\n",
    "    def _extract_boxed(self, text):\n",
    "        # Enhanced extraction\n",
    "        if not text: return None\n",
    "        for pattern in [r'\\\\boxed\\s*\\{\\s*([0-9]+)\\s*\\}', r'final\\s+answer\\s+is\\s*([0-9]+)']:\n",
    "            m = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if m: return int(m[-1])\n",
    "        return None\n",
    "\n",
    "    def _attempt(self, problem, role_prompt, idx, stop_evt, deadline):\n",
    "        if stop_evt.is_set() or time.time() > deadline: return None\n",
    "        \n",
    "        sandbox = self.sandbox_pool.get()\n",
    "        tool = AIMO3Tool(6, self.cfg.tool_prompt, sandbox)\n",
    "        \n",
    "        # Jitter temperature based on role\n",
    "        local_temp = self.cfg.temperature\n",
    "        if \"brute-force\" in role_prompt: local_temp = 1.0 # High creativity for code generation\n",
    "        if \"theoretical\" in role_prompt: local_temp = 0.7 # Strict for formalism\n",
    "        \n",
    "        msgs = self.template.apply_chat_template(role_prompt, problem, tool.tool_config)\n",
    "        conv = Conversation.from_messages(msgs)\n",
    "        \n",
    "        ans = None\n",
    "        entropy = 0.0\n",
    "        \n",
    "        try:\n",
    "            for i in range(self.cfg.turns):\n",
    "                if stop_evt.is_set() or time.time() > deadline: break\n",
    "                \n",
    "                prompt_ids = self.encoding.render_conversation_for_completion(conv, Role.ASSISTANT)\n",
    "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
    "                if max_tokens < 512: break\n",
    "                \n",
    "                stream = self.client.completions.create(\n",
    "                    model=self.cfg.served_model_name, prompt=prompt_ids, temperature=local_temp,\n",
    "                    max_tokens=max_tokens, stream=True, stop_token_ids=self.stop_ids, \n",
    "                    logprobs=5, min_p=self.cfg.min_p\n",
    "                )\n",
    "                \n",
    "                tok_buf, txt_buf = [], []\n",
    "                logprobs_buf = []\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    if stop_evt.is_set() or time.time() > deadline: break\n",
    "                    if chunk.choices[0].token_ids:\n",
    "                        tok_buf.extend(chunk.choices[0].token_ids)\n",
    "                        txt_buf.append(chunk.choices[0].text)\n",
    "                        if getattr(chunk.choices[0], 'logprobs', None):\n",
    "                           logprobs_buf.extend(chunk.choices[0].logprobs.top_logprobs)\n",
    "                           \n",
    "                    if '}' in chunk.choices[0].text:\n",
    "                         # Quick scan\n",
    "                         recent = \"\".join(txt_buf[-40:])\n",
    "                         if self._extract_boxed(recent) is not None:\n",
    "                             ans = self._extract_boxed(recent)\n",
    "                             break\n",
    "                \n",
    "                if ans is not None: break\n",
    "                if not tok_buf: break\n",
    "                \n",
    "                # Update entropy\n",
    "                if logprobs_buf:\n",
    "                    ents = [-sum(math.exp(v)*math.log(math.exp(v)) for v in d.values() if math.exp(v)>0) for d in logprobs_buf if d]\n",
    "                    entropy = sum(ents)/len(ents) if ents else 0\n",
    "                \n",
    "                new_msgs = self.encoding.parse_messages_from_completion_tokens(tok_buf, Role.ASSISTANT)\n",
    "                conv.messages.extend(new_msgs)\n",
    "                \n",
    "                last = new_msgs[-1]\n",
    "                if last.channel == 'final': \n",
    "                    ans = self._extract_boxed(last.content[0].text)\n",
    "                    break\n",
    "                \n",
    "                if last.recipient == 'python':\n",
    "                    resp = tool.process_sync_plus(last)\n",
    "                    conv.messages.extend(resp)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Err {idx}: {e}\")\n",
    "        finally:\n",
    "            sandbox.reset()\n",
    "            self.sandbox_pool.put(sandbox)\n",
    "            \n",
    "        return {'role': role_prompt[:20], 'answer': ans, 'entropy': entropy}\n",
    "\n",
    "    def solve(self, problem):\n",
    "        print(f\"Problem: {problem[:50]}...\")\n",
    "        \n",
    "        # Time Management\n",
    "        elapsed = time.time() - self.time_start\n",
    "        budget = max(self.cfg.base_problem_timeout, \n",
    "                    min(self.cfg.notebook_limit - elapsed - (self.problems_remaining-1)*self.cfg.base_problem_timeout, \n",
    "                        self.cfg.high_problem_timeout))\n",
    "        deadline = time.time() + budget\n",
    "        \n",
    "        results = []\n",
    "        valid_answers = []\n",
    "        stop_evt = threading.Event()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as ex:\n",
    "            futures = []\n",
    "            # Launch diversity attempts\n",
    "            num_attempts = min(self.cfg.attempts, len(self.cfg.active_roles))\n",
    "            for i in range(num_attempts):\n",
    "                role = self.cfg.active_roles[i]\n",
    "                futures.append(ex.submit(self._attempt, problem, role, i, stop_evt, deadline))\n",
    "            \n",
    "            # Monitor\n",
    "            for f in as_completed(futures):\n",
    "                try:\n",
    "                    r = f.result()\n",
    "                    if r: \n",
    "                        results.append(r)\n",
    "                        if r['answer'] is not None:\n",
    "                            valid_answers.append(r['answer'])\n",
    "                            # Early stopping if we have a strong consensus (e.g., 3 same answers)\n",
    "                            # But with diversity, we might want to wait a bit longer.\n",
    "                            c = Counter(valid_answers)\n",
    "                            if c.most_common(1)[0][1] >= 4:\n",
    "                                stop_evt.set()\n",
    "                except Exception as e:\n",
    "                    print(f\"Future fail: {e}\")\n",
    "                    \n",
    "        self.problems_remaining = max(0, self.problems_remaining - 1)\n",
    "        \n",
    "        # Diversity Voting\n",
    "        votes = defaultdict(float)\n",
    "        for r in results:\n",
    "            if r['answer'] is not None:\n",
    "                # Weights:\n",
    "                # - Theoretical/Formalist: 1.2x (Harder to fake)\n",
    "                # - Hacker: 1.1x (If code worked)\n",
    "                # - Architect: 1.0x\n",
    "                weight = 1.0\n",
    "                if \"theoretical\" in r['role']: weight = 1.3\n",
    "                if \"specialize\" in r['role']: weight = 1.1 # Hacker prompt\n",
    "                \n",
    "                # Entropy penalty\n",
    "                weight *= (1.0 / (1.0 + r['entropy']))\n",
    "                \n",
    "                votes[r['answer']] += weight\n",
    "        \n",
    "        if votes:\n",
    "            final = max(votes.items(), key=lambda x: x[1])[0]\n",
    "            print(f\"Final Answer: {final} (Votes: {len(valid_answers)})\")\n",
    "            return final\n",
    "        \n",
    "        print(\"Final Answer: 0 (No valid answers)\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = DiversitySolver(CFG)\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "    # Kaggle competition entry point\n",
    "    id_val = id_.item(0)\n",
    "    q_text = question.item(0)\n",
    "    \n",
    "    gc.collect()\n",
    "    final_ans = solver.solve(q_text)\n",
    "    gc.collect()\n",
    "    \n",
    "    return pl.DataFrame({'id': id_val, 'answer': final_ans})\n",
    "\n",
    "# Launch Inference Server\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
